{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.9.03 SVLoR for One Sample with Modularization\n",
    "\n",
    "PA 9.02에서 했던 SVLoR에서<br>\n",
    "Dataset을 생성할 때 다양한 조건들을 변경하면서<br>\n",
    "결과값을 비교합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.1\n",
    "PA 9.02에서 사용한 모듈들을 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import basic_nodes as nodes\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self):\n",
    "        self._feature_dim = 1\n",
    "        self._Th = None\n",
    "        \n",
    "        self.node_imp()\n",
    "        self.random_initialization()\n",
    "\n",
    "    def node_imp(self):\n",
    "        self._node1 = nodes.mul_node()\n",
    "        self._node2 = nodes.plus_node()\n",
    "    \n",
    "    def random_initialization(self):\n",
    "        r_feature_dim = 1/np.sqrt(self._feature_dim)\n",
    "        self._Th = np.random.uniform(low = -1 * r_feature_dim,\n",
    "                                     high = r_feature_dim,\n",
    "                                     size = (self._feature_dim + 1,1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self._z1 = self._node1.forward(self._Th[1], x)\n",
    "        self._z2 = self._node2.forward(self._Th[0], self._z1)\n",
    "\n",
    "        return self._z2\n",
    "    \n",
    "    def backward(self, dz, lr):\n",
    "        dth0, dz1 = self._node2.backward(dz)\n",
    "        dth1, dx = self._node1.backward(dz1)\n",
    "\n",
    "        self._Th[1] = self._Th[1] - lr*dth1\n",
    "        self._Th[0] = self._Th[0] - lr*dth0\n",
    "    \n",
    "    def get_Th(self):\n",
    "        return self._Th \n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self._pred = None\n",
    "    \n",
    "    def forward(self, z):\n",
    "        self._pred = 1/(1 + np.exp(-1*z))\n",
    "        return self._pred\n",
    "    \n",
    "    def backward(self, dpred):\n",
    "        partial = self._pred * (1 - self._pred)\n",
    "        dz = dpred * partial\n",
    "        return dz\n",
    "\n",
    "class BinaryCrossEntropy_Loss:\n",
    "    def __init__(self):\n",
    "        self._y, self._pred = None, None\n",
    "    \n",
    "    def forward(self, y, pred):\n",
    "        self._y, self._pred = y, pred\n",
    "        loss = -1*(y*np.log(self._pred) + (1-y)*np.log(1-pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        dpred = (self._pred - self._y)/(self._pred*(1-self._pred))\n",
    "        return dpred\n",
    "\n",
    "class SVLoR:\n",
    "    def __init__(self):\n",
    "        self._feature_dim = 1\n",
    "        self._affine = Affine()\n",
    "        self._sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._affine.forward(x)\n",
    "        pred =  self._sigmoid.forward(z)\n",
    "        return pred\n",
    "    \n",
    "    def backward(self, dpred, lr):\n",
    "        dz = self._sigmoid.backward(dpred)\n",
    "        self._affine.backward(dz, lr)\n",
    "        \n",
    "    def  get_Th(self):\n",
    "        return self._affine.get_Th()\n",
    "\n",
    "def result_tracker():\n",
    "    global iter_idx, check_freq\n",
    "    global th_accum, model\n",
    "\n",
    "    if iter_idx % check_freq == 0:\n",
    "        th_accum = np.hstack((th_accum, model.get_Th()))\n",
    "        loss_list.append(loss)\n",
    "    iter_idx += 1\n",
    "\n",
    "def result_visualizer():\n",
    "    global th_accum, loss_list\n",
    "    fig,ax = plt.subplots(2, 1, figsize = (30,10))\n",
    "    fig.subplots_adjust(hspace = 0.3)\n",
    "    ax[0].set_title(r'$\\vec{\\theta}$' + 'Update ')\n",
    "\n",
    "    ax[0].plot(th_accum[1,:], label = r'$\\theta_{1}$')\n",
    "    ax[0].plot(th_accum[0,:], label = r'$\\theta_{0}$')\n",
    "\n",
    "    ax[0].legend()\n",
    "    iter_ticks = np.linspace(0,th_accum.shape[1],10).astype(np.int)\n",
    "    ax[0].set_xticks(iter_ticks)\n",
    "\n",
    "    ax[1].set_title(r'$\\mathcal{L}$')\n",
    "    ax[1].plot(loss_list)\n",
    "    ax[1].set_xticks(iter_ticks)\n",
    "\n",
    "    n_pred = 1000\n",
    "    fig,ax = plt.subplots(figsize = (30,10))\n",
    "    ax.set_title('Predictor Update')\n",
    "    ax.scatter(data[:,1], data[:,-1])\n",
    "\n",
    "    ax_idx_arr = np.linspace(0,len(loss_list)-1,n_pred).astype(np.int)\n",
    "    cmap = cm.get_cmap('rainbow',lut = len(ax_idx_arr))\n",
    "\n",
    "    x_pred = np.linspace(np.min(data[:,1]),np.max(data[:,1]),1000)\n",
    "    for ax_cnt, ax_idx in enumerate(ax_idx_arr):\n",
    "        z = th_accum[1, ax_idx] * x_pred + th_accum[0,ax_idx]\n",
    "        a = 1/(1 + np.exp(-1 * z))\n",
    "        ax.plot(x_pred, a, color = cmap(ax_cnt),alpha = 0.2)\n",
    "\n",
    "    y_ticks = np.round(np.linspace(0, 1, 7),2)\n",
    "    ax.set_yticks(y_ticks)\n",
    "def dataset_generator(x_dict):\n",
    "    x_data = np.random.normal(x_dict['mean'], x_dict['std'],x_dict['n_sample'])\n",
    "    x_data_noise = x_data + x_dict['noise_factor'] * np.random.normal(0,1,x_dict['n_sample'])\n",
    "    \n",
    "    if x_dict['direction'] > 0:\n",
    "        y_data = (x_data_noise > x_dict['cutoff']).astype(np.int)\n",
    "    else:\n",
    "        y_data = (x_data_noise < x_dict['cutoff']).astype(np.int)\n",
    "    \n",
    "    data = np.zeros(shape = (x_dict['n_sample'],1))\n",
    "    data = np.hstack((data,x_data.reshape(-1,1),y_data.reshape(-1,1)))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.2\n",
    "비교를 위하여 PA 9.02에서 했던 값을 동일하게 진행합니다<br>\n",
    "\"\"\"<br>\n",
    "mean: 1<br>\n",
    "std: 1<br>\n",
    "n_sample: 300<br>\n",
    "noise_factor: 0.3<br>\n",
    "cutoff: 1<br>\n",
    "direction: -1<br>\n",
    "iter_idx: 0<br>\n",
    "check_freq: 5<br>\n",
    "epochs: 300<br>\n",
    "learning rate: 0.01<br>\n",
    "\"\"\"<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start\n",
    "x_dict =  {'mean': , 'std': , 'n_sample': ,\n",
    "            'noise_factor': ,\n",
    "            'cutoff': , 'direction': }\n",
    "\n",
    "data = \n",
    "\n",
    "model = \n",
    "BCE_loss = \n",
    "\n",
    "th_accum = \n",
    "\n",
    "loss_list = []\n",
    "iter_idx, check_freq = \n",
    "epochs, lr =  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        x,y =  \n",
    "\n",
    "        pred =\n",
    "        loss = \n",
    "\n",
    "        dpred = \n",
    "        model.backward(dpred, lr)\n",
    "### End\n",
    "        result_tracker()\n",
    "result_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** \n",
    "<img src='./img_9/9_05.png' width = 700>\n",
    "<img src='./img_9/9_06.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.3\n",
    "mean과 cutoff를 10으로 증가시키고 실행하여<br>\n",
    "나머지 조건을 동일하게 하고 결과값을 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start\n",
    "x_dict =  {'mean': , 'std': , 'n_sample': ,\n",
    "            'noise_factor': ,\n",
    "            'cutoff': , 'direction': }\n",
    "\n",
    "data = \n",
    "\n",
    "model = \n",
    "BCE_loss = \n",
    "\n",
    "th_accum =\n",
    "\n",
    "loss_list = []\n",
    "iter_idx, check_freq = \n",
    "epochs, lr =  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        x,y =\n",
    "\n",
    "        pred = \n",
    "        loss =\n",
    "\n",
    "        dpred = \n",
    "        model.backward(dpred, lr)\n",
    "### End\n",
    "        result_tracker()\n",
    "result_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** \n",
    "<img src='./img_9/9_07.png' width = 700>\n",
    "<img src='./img_9/9_08.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.4\n",
    "Step.3에서는 data의 mean값이 증가하여서 x1와 x0가 동일하게 학습이 되지 않는 모습을<br>\n",
    "확인할 수 있는데 따라서 동일 epochs에서 명확한 solution을 얻기 힘듭니다<br>\n",
    "따라서 epochs를 늘려서 학습하면 어떤 결과가 나오는지 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start\n",
    "x_dict =  {'mean': , 'std': , 'n_sample': ,\n",
    "            'noise_factor': ,\n",
    "            'cutoff': , 'direction': }\n",
    "\n",
    "data = \n",
    "\n",
    "model = \n",
    "BCE_loss = \n",
    "\n",
    "th_accum = \n",
    "\n",
    "loss_list = []\n",
    "iter_idx, check_freq = \n",
    "epochs, lr = \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        x,y =  \n",
    "\n",
    "        pred = \n",
    "        loss = \n",
    "\n",
    "        dpred =\n",
    "        model.backward(dpred, lr)\n",
    "### End\n",
    "        result_tracker()\n",
    "result_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** \n",
    "<img src='./img_9/9_09.png' width = 700>\n",
    "<img src='./img_9/9_10.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.5\n",
    "Step.3, 4에서는 mean값을 증가시켜 학습결과를 관찰했습니다<br>\n",
    "Step.5에서는 mean값은 초기와 같이 1로 설정하고 std를 변경시켜<br>\n",
    "학습 결과를 확인할 것입니다<br>\n",
    "mean:1<br>\n",
    "std:7<br>\n",
    "cutoff:1<br>\n",
    "epochs:1000<br>\n",
    "learning rate:0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start\n",
    "x_dict =  {'mean': , 'std': , 'n_sample': ,\n",
    "            'noise_factor': ,\n",
    "            'cutoff': , 'direction': }\n",
    "\n",
    "data = \n",
    "\n",
    "model = \n",
    "BCE_loss =\n",
    "\n",
    "th_accum = \n",
    "\n",
    "loss_list = []\n",
    "iter_idx, check_freq = \n",
    "epochs, lr = \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        x,y = \n",
    "\n",
    "        pred = \n",
    "        loss = \n",
    "\n",
    "        dpred = \n",
    "        model.backward(dpred, lr)\n",
    "### End\n",
    "        result_tracker()\n",
    "result_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** \n",
    "<img src='./img_9/9_11.png' width = 700>\n",
    "<img src='./img_9/9_12.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Std를 증가시키게 되면<br>\n",
    "mean에서와 비슷하게 x1와 x0의 학습이 불균형하게 일어나는 모습을 볼수 있다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
