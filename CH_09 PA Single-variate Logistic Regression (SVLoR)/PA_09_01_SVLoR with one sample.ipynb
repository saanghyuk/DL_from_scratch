{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.9.01 SVLoR for One Sample\n",
    "\n",
    "SVLoR을 One sample을 통해서 진행하여<br>\n",
    "decision boundary가 어떻게 형성되는지 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import basic_nodes as nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.1\n",
    "먼저 데이터 생성을 위하여 dataset_generator함수를 구현한다<br>\n",
    "input으로 mean, std, n_sample, noise_factor, cutoff, direction,이 포함된<br>\n",
    "x_dict를 받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(x_dict):\n",
    "    ### Start\n",
    "    x_data = np.random.normal()\n",
    "    x_data_noise = x_data + \n",
    "    if x_dict['direction'] > 0:\n",
    "        y_data = (x_data_noise > x_dict['cutoff']).astype(np.int)\n",
    "    else:\n",
    "        y_data = (x_data_noise < x_dict['cutoff']).astype(np.int)\n",
    "    \n",
    "    data = np.zeros()\n",
    "    data = np.hstack()\n",
    "    ### End\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.2\n",
    "dataset_generator을 통하여 Dataset을 생성하는데<br><br>\n",
    "\"\"\"<br>\n",
    "mean: 1<br>\n",
    "std: 1<br>\n",
    "n_sample: 300<br>\n",
    "noise_factor: 0.3<br>\n",
    "cutoff: 1<br>\n",
    "direction: 1<br>\n",
    "\"\"\"<br><br>\n",
    "위와 같이 x_dict를 생성하여 dataset_generator를 실행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {'mean': ,\n",
    "          'std': ,\n",
    "          'n_sample': ,\n",
    "          'noise_factor': ,\n",
    "          'cutoff': ,\n",
    "          'direction': }\n",
    "data = dataset_generator(x_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.3\n",
    "먼저 mul_node와 plus_node를 선언합니다<br>\n",
    "그리고 필요한 변수들을 정의합니다<br>\n",
    "iter_idx는 0으로 초기화하고 확인을 위해 사용할 check_freq는 5마다 체크합니다<br>\n",
    "epoch는 100으로 설정하고 lr은 0.1로 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = nodes.mul_node()\n",
    "node2 = nodes.plus_node()\n",
    "### Start\n",
    "Th = \n",
    "th_accum = Th\n",
    "\n",
    "loss_list = []\n",
    "iter_idx, check_freq = \n",
    "epochs, lr = \n",
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for data_idx in range(data.shape[0]):\n",
    "        ### Start\n",
    "        x,y = \n",
    "        \n",
    "        z1 = \n",
    "        z2 = \n",
    "        pred = \n",
    "        \n",
    "        loss = \n",
    "        \n",
    "        dpred = \n",
    "        dz2 = \n",
    "        dth0, dz1 = \n",
    "        dth1, dx =\n",
    "        \n",
    "        Th[1] = \n",
    "        Th[0] = \n",
    "        ### End\n",
    "        \n",
    "        if iter_idx % check_freq == 0:\n",
    "            th_accum = np.hstack((th_accum, Th))\n",
    "            loss_list.append(loss)\n",
    "        iter_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.5 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize = (30,10))\n",
    "fig.subplots_adjust(hspace = 0.3)\n",
    "ax[0].set_title(r'$\\vec{\\theta}$' + 'Update')\n",
    "\n",
    "ax[0].plot(th_accum[1,:], label = r'$\\theta_{1}$')\n",
    "ax[0].plot(th_accum[0,:], label = r'$\\theta_{0}$')\n",
    "\n",
    "ax[0].legend()\n",
    "iter_ticks = np.linspace(0, th_accum.shape[1],10).astype(np.int)\n",
    "ax[0].set_xticks(iter_ticks)\n",
    "\n",
    "ax[1].set_title('Loss Decrease')\n",
    "ax[1].plot(loss_list)\n",
    "ax[1].set_xticks(iter_ticks)\n",
    "\n",
    "n_pred = 1000\n",
    "fig,ax = plt.subplots(figsize = (30,10))\n",
    "ax.set_title('Predictor Update')\n",
    "ax.scatter(data[:,1],data[:,-1])\n",
    "\n",
    "ax_idx_arr = np.linspace(0, len(loss_list)-1, n_pred).astype(np.int)\n",
    "cmap = cm.get_cmap('rainbow',lut = len(ax_idx_arr))\n",
    "\n",
    "x_pred = np.linspace(np.min(data[:,1]),np.max(data[:,1]),1000)\n",
    "\n",
    "for ax_cnt,ax_idx in enumerate(ax_idx_arr):\n",
    "    z = th_accum[1, ax_idx]*x_pred + th_accum[0, ax_idx]\n",
    "    a = 1/(1 + np.exp(-1*z))\n",
    "    ax.plot(x_pred,a,color = cmap(ax_cnt),alpha = 0.2)\n",
    "y_ticks = np.round(np.linspace(0,1,7),2)\n",
    "ax.set_yticks(y_ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** \n",
    "<img src='./img_9/9_01.png' width = 700>\n",
    "<img src='./img_9/9_02.png' width = 700>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
