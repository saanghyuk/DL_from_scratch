{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.5 Multi-variate Linear Regression\n",
    "\n",
    "## Programming Assignment.5-01 Gradient Descent Method\n",
    "\n",
    "기존 Chapter4에서 사용했던 기존의 Model\n",
    "$$\\hat{y} = \\theta_{1}x + \\theta_{0}$$\n",
    "에서 여러개의 변수를 사용하게 되는데\n",
    "2개부터 적용하게 되면 Model은 다음과 같이\n",
    "$$\\hat{y} = \\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}$$\n",
    "가 됩니다.\n",
    "\n",
    "따라서 이번 PA 5-01에서는 다음의 단계들을 통해 $$\\hat{y} = \\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}$$에 대해 gradient descent method를 적용하여 $\\theta_{2}, \\theta_{1}, \\theta_{0}$을 학습시킵니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Preparation </li>\n",
    "    <li> Step.2 One Iteration of GDM </li>\n",
    "    <li> Step.3 Gradient Descent Method </li>\n",
    "    <li> Step.4 Predictor Visualization </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Preparation\n",
    "\n",
    "MVLR을 위한 dataset은\n",
    "$$y = ax_{2} + bx_{1} + c$$\n",
    "에서부터 만들어집니다\n",
    "\n",
    "$$y = 2x_{2} - 1x_{1} + 3$$\n",
    "에서 200개의 data sample을 가지는 dataset을 만드는 코드입니다\n",
    "<br> mean의 경우 0으로 지정하고 standard deviation은 1로 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_sample = 200\n",
    "coefficient_list = [3, -1, 2]\n",
    "\n",
    "data_gen = LR_dataset_generator(feature_dim = 2)\n",
    "data_gen.set_n_sample(n_sample)\n",
    "distribution_params = {\n",
    "    0: {'mean':0, 'std':1},\n",
    "    1: {'mean':0, 'std':1}\n",
    "}\n",
    "data_gen.set_coefficient(coefficient_list)\n",
    "x_data, y_data = data_gen.make_dataset()\n",
    "dataset = np.hstack((x_data, y_data))\n",
    "\n",
    "print(dataset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "(200, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위의 코드를 이용하여\n",
    "$$y = 5x_{2} - 3x_{1} + 7$$\n",
    "에서부터 700개의 data sample을 가지는 dataset을 만드세요.<br>\n",
    "mean은 0으로 standard deviation는 1로 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "n_sample = \n",
    "coefficient_list = \n",
    "\n",
    "##### End Your Code(Learning Preparation) #####\n",
    "x_data, y_data = data_gen.make_dataset()\n",
    "dataset = np.hstack((x_data, y_data))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "(700, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 One Iteration of GDM\n",
    "\n",
    "SVLR의 predictor는 다음과 같다.\n",
    "$$\n",
    "\\hat{y} = \\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}\n",
    "$$\n",
    "\n",
    "따라서 1개의 data sample에 대한 loss는\n",
    "\n",
    "$$ \\mathcal{L}^{(i)} = (y^{(i)} - \\hat{y}^{(i)})^{2} = (y^{(i)} - (\\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}))^{2}$$\n",
    "\n",
    "이므로 $\\mathcal{L}$은 $\\theta_{2}, \\theta_{1}, \\theta_{0}$에 대한 함수이다.  \n",
    "이때 각각 $\\theta_{2}, \\theta_{1}, \\theta_{0}$에 대한 partial derivative를 구하면\n",
    "$$\n",
    "\\frac {\\partial \\mathcal{L}^{(i)}} {\\partial \\theta_{2}}\n",
    "= -2x_{2}^{(i)}(y^{(i)} - (\\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial \\mathcal{L}^{(i)}} {\\partial \\theta_{1}}\n",
    "= -2x_{1}^{(i)}(y^{(i)} - (\\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial \\mathcal{L}^{(i)}} {\\partial \\theta_{0}}\n",
    "= -2(y^{(i)} - (\\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}))\n",
    "$$\n",
    "\n",
    "이다. 위의 partial derivative를 이용하여 GDM을 적용하면 $\\theta_{1}, \\theta_{0}$은 다음과 같이 update됩니다.\n",
    "\n",
    "$$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta} \n",
    "= \\theta_{1} + 2\\alpha x^{(i)}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "$$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta} \n",
    "= \\theta_{0} + 2\\alpha(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "data sample에 대하여 한 번의 iteration을 연산해보세요.  \n",
    "이때 학습 조건은 다음과 같습니다.\n",
    "- initial theta2, theta1, theta0 = 1, 1\n",
    "- learning rate = 0.01\n",
    "- $x_{2}$, $x_{1}$,  y = 2, 1, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Initial Setting) #####\n",
    "th2, th1, th0 = \n",
    "lr = \n",
    "x2 = \n",
    "x1 = \n",
    "y = \n",
    "\n",
    "##### End Your Code(Initial Setting) #####\n",
    "print(\"Before Update:\", th2, th1, th0)\n",
    "\n",
    "\n",
    "##### Start Your Code(Partial Derivative Calculation) #####\n",
    "pred = \n",
    "dth2 = \n",
    "dth1 = \n",
    "dth0 =\n",
    "##### Start Your Code(Partial Derivative Calculation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Gradient Descent Method) #####\n",
    "th2 = \n",
    "th1 = \n",
    "th0 = \n",
    "##### Start Your Code(Gradient Descent Method) #####\n",
    "print(\"After Update:\", th2, th1, th0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Before Update: 1 1 1 \n",
    "<br>After Update: 1.08 1.04 1.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Gradient Descent Method\n",
    "\n",
    "Step.3에서는 dataset에 들어있는 data sample들을 이용하여 $\\theta_{2}, \\theta_{1}, \\theta_{0}$를 학습시킵니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "PA 3-01과 마찬가지로 for loop을 이용하여 data sample에 접근하고, 각 data sample에 대해 GDM을 적용하여 $\\theta_{2}, \\theta_{1}, \\theta_{0}를 학습시키세요.$  \n",
    "이때 학습 조건은 다음과 같습니다.\n",
    "- $\\theta_{2}, \\theta_{1}, \\theta_{0} = 0.1, 0.1$\n",
    "- learning rate = 0.01\n",
    "- epochs = 3\n",
    "\n",
    "학습이 끝나면 target function $y = 5x_{2} - 3x_{1} + 7$에 가까워지도록 $\\theta_{1}, \\theta_{0}$가 학습되는지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th2, th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th2_list, th1_list, th0_list = [], [], []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_sample in dataset:\n",
    "        x2, x1, y = data_sample[2], data_sample[1], data_sample[-1]\n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        pred = \n",
    "        loss = \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        th2_list.append(th2)\n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th2 = \n",
    "        th1 = \n",
    "        th0 = \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th2_list, label = r'$\\theta_{2}$')\n",
    "ax[0].plot(th1_list, label = r'$\\theta_{1}$')\n",
    "ax[0].plot(th0_list, label = r'$\\theta_{0}$')\n",
    "ax[0].legend(loc = 'upper left', fontsize = 30)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/5_01_1.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step.3의 결과를 보게 되면 mean을 0으로 standard deviation을 1로 맞추었기 때문에<br>\n",
    "$\\theta$들간의 학습이 동일하게 되는 것을 볼 수 있다<br>\n",
    "이제부터는 mean과 std를 변화시키면서 학습 결과를 관찰해 볼것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위에서 진행했던 것과 동일하게 진행하며\n",
    "$$y = 5x_{2} - 3x_{1} + 7$$\n",
    "에서부터 700개의 data sample을 가지는 dataset을 만드세요.<br><br>\n",
    "x2의 mean은 0으로 standard deviation는 3으로 설정합니다<br>\n",
    "x1의 mean은 0 standard deviation은 그대로 1로 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "n_sample = \n",
    "coefficient_list = []\n",
    "\n",
    "##### End Your Code(Learning Preparation) #####\n",
    "x_data, y_data = data_gen.make_dataset()\n",
    "dataset = np.hstack((x_data, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning and visualization 과정은 위에서와 동일하게 진행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th2, th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th2_list, th1_list, th0_list = [], [], []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_sample in dataset:\n",
    "        x2, x1, y = data_sample[2], data_sample[1], data_sample[-1]\n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        pred =\n",
    "        loss = \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        th2_list.append(th2)\n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th2 = \n",
    "        th1 =\n",
    "        th0 = \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th2_list, label = r'$\\theta_{2}$')\n",
    "ax[0].plot(th1_list, label = r'$\\theta_{1}$')\n",
    "ax[0].plot(th0_list, label = r'$\\theta_{0}$')\n",
    "ax[0].legend(loc = 'upper left', fontsize = 30)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/5_01_2.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x2의 std를 3으로 설정하였기 때문에\n",
    "$\\theta_{2}$가 다른 $\\theta$들보다 먼저 학습이 진행된 것을 볼수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 Mean을 변경해볼 것이다<br>\n",
    "x2의 mean은 3으로 standard deviation는 1으로 설정하고<br>\n",
    "x1의 mean은 0 standard deviation은 그대로 1로 설정합니다<br>\n",
    "이번 코드에서는 epoch가 3으로 부족해서 10으로 진행하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "n_sample =\n",
    "coefficient_list = \n",
    "\n",
    "##### End Your Code(Learning Preparation) #####\n",
    "x_data, y_data = data_gen.make_dataset()\n",
    "dataset = np.hstack((x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th2, th1, th0 = 0.1, 0.1, 0.1\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th2_list, th1_list, th0_list = [], [], []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_sample in dataset:\n",
    "        x2, x1, y = data_sample[2], data_sample[1], data_sample[-1]\n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        pred = th2*x2 + th1*x1 + th0\n",
    "        loss = np.power(y - pred, 2)\n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        th2_list.append(th2)\n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th2 = th2 + 2*x2*lr*(y - pred)\n",
    "        th1 = th1 + 2*x1*lr*(y - pred)\n",
    "        th0 = th0 + 2*lr*(y - pred)\n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th2_list, label = r'$\\theta_{2}$')\n",
    "ax[0].plot(th1_list, label = r'$\\theta_{1}$')\n",
    "ax[0].plot(th0_list, label = r'$\\theta_{0}$')\n",
    "ax[0].legend(loc = 'upper left', fontsize = 30)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/5_01_3.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean을 변경시키는 경우 같은 epoch안에 학습이 끝나지 않았고<br>\n",
    "3배 가량의 epoch동안 학습을 해야<br>\n",
    "정상적으로 학습이 완료되는 것을 확인할수 있습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ML': conda)",
   "language": "python",
   "name": "python37664bitmlconda63d4d036f3b243269c474d4202f7e655"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
