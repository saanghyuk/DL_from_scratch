{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.5 Multi-variate Linear Regression\n",
    "\n",
    "## Programming Assignment.5-04 MVLR for One Sample\n",
    "MVLR을 학습시키는데 앞에 있는 PA와 달리 basic building node를 이용하여 학습합니다\n",
    "model은\n",
    "$$\\hat{y} =\\theta_{2}x_{2} + \\theta_{1}x_{1} + \\theta_{0}$$\n",
    "\n",
    "이고, loss는 square error를 사용하기 때문에\n",
    "$$\\mathcal{L} = (y - \\hat{y})^{2}$$\n",
    "가 됩니다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import basic_nodes as nodes \n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 dataset이\n",
    "$$y = 4x_{2} + 3x_{1}  - 1$$\n",
    "에서부터 만들어지고, $x_{2}$,$x_{1}$는 standard normal distribution에서부터 만들어지는 코드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coefficient_list = [-1, 3, 4]\n",
    "\n",
    "data_gen = LR_dataset_generator(feature_dim = 2, n_sample = 150)\n",
    "data_gen.set_coefficient(coefficient_list)\n",
    "distribution_params = {0:{'mean':0, 'std':1},\n",
    "                       1:{'mean':0, 'std':1}\n",
    "                      }\n",
    "data_gen.set_distribution_params(distribution_params)\n",
    "x_data, y_data = data_gen.make_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 필요한 model과 loss function을 basic building node들을 이용하여 구현합니다.  \n",
    "강의에서 사용된 model과 loss는 다음과 같습니다.\n",
    "<img src='./imgs/5_4_1.jpg' width = 600>\n",
    "\n",
    "따라서 model을 implementation하기 위해선 mul_node, plus_node가 필요하고,  \n",
    "loss를 implementation하기 위해선 minus_node와 square_node가 필요합니다.\n",
    "\n",
    "node1에는 mul_node 배열을<br>\n",
    "node2에는 plus_node 배열을<br>\n",
    "node3에는 minus_node<br>\n",
    "node4에는 square_node를 구현합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 2\n",
    "node1 = [None] + [nodes.mul_node() for _ in range(feature_dim)]\n",
    "node2 = [None] + [nodes.plus_node() for _ in range(feature_dim)]\n",
    "node3 = nodes.minus_node()\n",
    "node4 = nodes.square_node()\n",
    "\n",
    "\n",
    "# Learning\n",
    "th_list = [0.1, 0.1, 0.1]\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 3\n",
    "th_accum = np.array(th_list).reshape(-1, 1)\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_idx, (X, y) in enumerate(zip(x_data, y_data)):\n",
    "        # Forward Propagation\n",
    "        z1_1 =\n",
    "        z1_2 =\n",
    "        \n",
    "        \n",
    "        z2_1 =\n",
    "        z2_2 =\n",
    "        \n",
    "        z3 =\n",
    "        loss =\n",
    "        # Forward Propagation end\n",
    "        dz3 =\n",
    "        dy, dz2_2 =\n",
    "        \n",
    "        dz2_1, dz1_2 = node2[2].backward(dz2_2)\n",
    "        dth0, dz1_1 = node2[1].backward(dz2_1)\n",
    "        \n",
    "    \n",
    "        dth2, dx2 = node1[2].backward(dz1_2)\n",
    "        dth1, dx1 = node1[1].backward(dz1_1)\n",
    "        \n",
    "        th_list[2] = th_list[2] - lr * dth2\n",
    "        th_list[1] = th_list[1] - lr * dth1\n",
    "        th_list[0] = th_list[0] - lr * dth0\n",
    "        \n",
    "        th_current = np.array(th_list).reshape(-1, 1)\n",
    "        th_accum = np.hstack((th_accum, th_current))\n",
    "        loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (40,20))\n",
    "for i in range(feature_dim + 1):\n",
    "    ax[0].plot(th_accum[i], label = r'$\\theta_{%d}$'%i,\n",
    "              linewidth = 5)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].legend(loc = 'lower right', fontsize = 30)\n",
    "ax[0].tick_params(axis = 'both', labelsize = 30)\n",
    "ax[1].tick_params(axis = 'both', labelsize = 39)\n",
    "\n",
    "ax[0].set_title(r'$\\vec{\\theta}$', fontsize = 40)\n",
    "ax[1].set_title('Loss', fontsize = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/5_04_2.png' width = 800>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ML': conda)",
   "language": "python",
   "name": "python37664bitmlconda63d4d036f3b243269c474d4202f7e655"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
