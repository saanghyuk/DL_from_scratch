{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.4 Single-variate Linear Regression\n",
    "\n",
    "## Programming Assignment.4-08 SVLR for Several Samples\n",
    "\n",
    "PA 4-08d에서는 data sample을 여러개 사용하는 mini-batch를 이용해서 $\\theta_{1}, \\theta_{0}$를 update하는 방법을 다룹니다.\n",
    "\n",
    "Chapter.3와 마찬가지로 이미 PA 4-07에서 만든 학습모델이 mini-batch에 대해서도 사용 가능하기 때문에 node들의 input, output diemnsion을 살펴보고 학습을 시켜봅니다.\n",
    "\n",
    "\n",
    "PA 4-08는 다음과 같은 단계들로 이루어집니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Preparation </li>\n",
    "    <li> Step.2 Model/Cost Implementation </li>\n",
    "    <li> Step.3 Dimension Check </li>\n",
    "    <li> Step.4 Learning </li>\n",
    "    <li> Step.5 Learning with Various Batch Sizes </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "import basic_nodes as nodes\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)\n",
    "\n",
    "def get_data_batch(dataset, batch_idx, batch_size, n_batch):\n",
    "    if batch_idx is n_batch -1:\n",
    "        batch = dataset[batch_idx*batch_size:]\n",
    "    else:\n",
    "        batch = dataset[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Preparation\n",
    "\n",
    "dataset은 PA 4-07과 동일하게 만들어줍니다.  \n",
    "\n",
    "PA 4-07과 마찬가지로 다양한 dataset이 학습에 어떠한 영향을 미치는지 살펴보기 위해 distribution parameter들을 설정해줍니다.  \n",
    "이를 통해 dataset의 전체적인 특성에 따라 cost를 이용해도 $J = \\theta_{1}^{2} + \\theta_{0}^{2}$에 가까운 모습을 만들지 못하는 현상을 분석합니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "다음의 조건을 만족하는 dataset을 만드세요.\n",
    "\n",
    "- target function = $y = 3x + 3$\n",
    "- mean, std of x = 0, 1\n",
    "\n",
    "(Hint.1) set_coefficient method  \n",
    "(Hint.2) set_distribution_params method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "##### Start Your Code(Dataset Setting) #####\n",
    "coefficient_list = \n",
    "distribution_params = \n",
    "##### End Your Code(Dataset Setting) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Dataset Generation) #####\n",
    "data_gen = \n",
    "data_gen.\n",
    "data_gen.\n",
    "dataset = \n",
    "##### End Your Code(Dataset Generation) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Model/Cost Implementation\n",
    "\n",
    "강의에서 설명한 mini-batch에 대한 학습은 다음과 같습니다.\n",
    "\n",
    "<img src='./imgs/4_08_01.png' width=900>\n",
    "\n",
    "PA 4-07에서 달라지는 것은 forward propagation에서는 loss들의 평균을 구해주고,  \n",
    "backpropagation에서는 n개의 path에 대해 각각 $\\frac{1}{n}$을 곱해주는 역할을 하는 mean_node입니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "위의 model과 cost를 basic building node를 이용해서 instantiation하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Model Implementation) #####\n",
    "node1 = \n",
    "node2 = \n",
    "##### End Your Code(Model Implementation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Loss Implementation) #####\n",
    "node3 = \n",
    "node4 = \n",
    "node5 = \n",
    "##### End Your Code(Loss Implementation) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Dimension Check\n",
    "\n",
    "Step.3에선 Stpe.2에서 instantiation한 model과 cost function에 8개의 data sample로 만든 mini-batch를 입력하여 forward/backward propagation에서 일어나는 값들의 dimension을 살펴봅니다.\n",
    "\n",
    "다음은 data를 mini-batch로 만드는 과정입니다.  \n",
    "다음 셀을 실행시켜 data의 dimension을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape :  (8, 3)\n",
      "X.shape :  (8,)\n",
      "Y.shape :  (8,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "th1, th0 = 0.1, 0.1\n",
    "batch_size = 8\n",
    "batch_idx = 0\n",
    "n_batch = int(np.ceil(dataset.shape[0]/batch_size))\n",
    "\n",
    "batch = get_data_batch(dataset, batch_idx)\n",
    "print(\"batch.shape : \", batch.shape)\n",
    "X, Y = batch[:,1], batch[:,-1]\n",
    "print(\"X.shape : \", X.shape)\n",
    "print(\"Y.shape : \", Y.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 forward propagation에서 전달되는 값들의 dimension을 나타낸 코드입니다.\n",
    "\n",
    "다음 셀을 실행하여 Z1부터 J까지의 dimension을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1.shape :  (8,)\n",
      "Z2.shape :  (8,)\n",
      "Z3.shape :  (8,)\n",
      "Z4.shape :  (8,)\n",
      "J.shape :  () \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z1 = node1.forward(th1, X)\n",
    "Z2 = node2.forward(th0, Z1)\n",
    "Z3 = node3.forward(Y, Z2)\n",
    "Z4 = node4.forward(Z3)\n",
    "J = node5.forward(Z4)\n",
    "\n",
    "print(\"Z1.shape : \", Z1.shape)\n",
    "print(\"Z2.shape : \", Z2.shape)\n",
    "print(\"Z3.shape : \", Z3.shape)\n",
    "print(\"Z4.shape : \", Z4.shape)\n",
    "print(\"J.shape : \", J.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 backpropagation에서 전달되는 값들의 dimension을 나타낸 코드입니다.\n",
    "\n",
    "다음 셀을 실행하여 dZ4부터 dTh0, dTh1까지 전달되는 값들의 dimension을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ4.shape :  (8,)\n",
      "dZ3.shape :  (8,)\n",
      "dZ2.shape :  (8,)\n",
      "dZ1.shape :  (8,)\n",
      "dTh1.shape :  (8,)\n",
      "dTh0.shape :  (8,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dZ4 = node5.backward(1)\n",
    "dZ3 = node4.backward(dZ4)\n",
    "_, dZ2 = node3.backward(dZ3)\n",
    "dTh0, dZ1 = node2.backward(dZ2)\n",
    "dTh1, _ = node1.backward(dZ1)\n",
    "\n",
    "print(\"dZ4.shape : \", dZ4.shape)\n",
    "print(\"dZ3.shape : \", dZ3.shape)\n",
    "print(\"dZ2.shape : \", dZ2.shape)\n",
    "print(\"dZ1.shape : \", dZ1.shape)\n",
    "print(\"dTh1.shape : \", dTh1.shape)\n",
    "print(\"dTh0.shape : \", dTh0.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 아직 vector로 dTh1, dTh0에 들어있는 partial derivative들을 더하여  \n",
    "dth1, dth0를 구하는 과정입니다.\n",
    "\n",
    "다음 셀을 실행하여 dth1, dth0가 scalar로 바뀌는 것을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dth1.shape :  ()\n",
      "dth0.shape :  ()\n"
     ]
    }
   ],
   "source": [
    "dth1 = np.sum(dTh1)\n",
    "dth0 = np.sum(dTh0)\n",
    "\n",
    "print(\"dth1.shape : \", dth1.shape)\n",
    "print(\"dth0.shape : \", dth0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Learning\n",
    "\n",
    "Step.4에서는 Step.3까지의 과정을 이용하여 mini-batch를 이용한 $\\theta_{1}, \\theta_{0}$의 학습을 다룹니다.\n",
    "\n",
    "먼저 다음의 학습환경을 설정해주세요.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "- initial $\\theta_{1}, \\theta_{0}$ = 0.1, 0.1\n",
    "- learning rate = 0.01\n",
    "- total epoch = 20\n",
    "- batch size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "\n",
    "batch_size = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_list, th0_list = [], []\n",
    "cost_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 완성하여 mini-batch를 이용한 forward propagation, backpropagation, gradient descent method를 완성하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = int(np.ceil(dataset.shape[0]/batch_size))\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(dataset)\n",
    "    for batch_idx in range(n_batch):\n",
    "        ##### Start Your Code(Batch Extraction) #####\n",
    "        batch = \n",
    "        X, Y = \n",
    "        ##### Start Your Code(Batch Extraction) #####\n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        Z1 = \n",
    "        Z2 = \n",
    "        Z3 = \n",
    "        Z4 = \n",
    "        J = \n",
    "        ##### End Your Code(Forward Propagation) #####\n",
    "        \n",
    "        \n",
    "        ##### Start Your Code(Backpropagation) #####\n",
    "        dZ4 = \n",
    "        dZ3 = \n",
    "        _, dZ2 = \n",
    "        dTh0, dZ1 = \n",
    "        dTh1, _ = \n",
    "        ##### End Your Code(Backpropagation) #####\n",
    "        \n",
    "        \n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "        cost_list.append(J)\n",
    "        \n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        dth1 = \n",
    "        dth0 = \n",
    "        \n",
    "        th1 = \n",
    "        th0 = \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (15,8))\n",
    "fig.subplots_adjust(hspace = 0.3)\n",
    "ax[0].plot(th1_list)\n",
    "ax[0].plot(th0_list)\n",
    "ax[1].plot(cost_list)\n",
    "ax[0].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[1].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_02.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드들을 이용하여 trainer, reulst_visualization 함수들을 만들어줍니다. \n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "result_visualization은 이미 만들어져있기 때문에, trainer의 내부를 완성하세요.\n",
    "\n",
    "이때 trainer의 input/output은 다음과 같습니다.\n",
    "\n",
    "- INPUT : dataset, th1, th0, lr, epochs, batch_size\n",
    "- OUTPUT : th1_list, th0_list, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataset, th1, th0, lr, epochs, batch_size):\n",
    "    ##### Start Your Code(trainer Functionalization) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(trainer Functionalization) #####\n",
    "    return th1_list, th0_list, cost_list\n",
    "\n",
    "def result_visualizer(th1_list, th0_list, cost_list):\n",
    "    fig, ax = plt.subplots(2, 1, figsize = (15,8))\n",
    "    fig.subplots_adjust(hspace = 0.3)\n",
    "    ax[0].plot(th1_list)\n",
    "    ax[0].plot(th0_list)\n",
    "    ax[1].plot(cost_list)\n",
    "    ax[0].tick_params(axis = 'both', labelsize = 20)\n",
    "    ax[1].tick_params(axis = 'both', labelsize = 20)\n",
    "    ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "    ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "    \n",
    "th1, th0 = 0.1, 0.1\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "batch_size = 2\n",
    "th1_list, th0_list, cost_list = trainer(dataset, th1, th0, lr, epochs, batch_size)\n",
    "result_visualizer(th1_list, th0_list, cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_03.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 두 함수를 이용하여 학습을 시키면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th1, th0 = 0.1, 0.1\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "batch_size = 12\n",
    "th1_list, th0_list, cost_list = trainer(dataset, th1, th0, lr, epochs, batch_size)\n",
    "result_visualizer(th1_list, th0_list, cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_04.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.5 Learning with Various Batch Sizes\n",
    "\n",
    "Step.4에서 만든 함수 2개를 이용하여 batch size가 학습에 미치는 영향을 살펴봅니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "\n",
    "다음의 조건\n",
    "- initial $\\theta_{1}, \\theta_{0}$ = 0.1, 0.1\n",
    "- learning rate = 0.01\n",
    "\n",
    "\n",
    "은 고정으로 두고, batch size를 변화시켜가며 결과를 확인하고 결과가 나온 이유를 분석하세요.  \n",
    "이때 batch size는 epoch당 iteration에 변화를 만드므로 epoch도 알맞게 조절해줘야합니다. \n",
    "\n",
    "- batch size = 1, 2, 8, 16, 32, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "\n",
    "batch_size = \n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "\n",
    "\n",
    "# Training and Result Visualization\n",
    "th1_list, th0_list, cost_list = trainer(dataset, th1, th0, lr, epochs, batch_size)\n",
    "result_visualizer(th1_list, th0_list, cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_05.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "\n",
    "다음의 조건\n",
    "- initial $\\theta_{1}, \\theta_{0}$ = 0.1, 0.1\n",
    "- learning rate = 0.01\n",
    "\n",
    "\n",
    "은 고정으로 두고, dataset의 std와 batch size를 변화시켜가며 학습이 되는 경향을 확인하세요.  \n",
    "이때 batch size는 epoch당 iteration에 변화를 만드므로 epoch도 알맞게 조절해줘야합니다. \n",
    "\n",
    "- mean = 0\n",
    "- std = 1, 2, 3, 4\n",
    "- batch size = 1, 2, 8, 16, 32, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "\n",
    "batch_size = \n",
    "\n",
    "coefficient_list = \n",
    "distribution_params = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "\n",
    "# Dataset Generation\n",
    "data_gen = LR_dataset_generator(feature_dim = 1)\n",
    "data_gen.set_coefficient(coefficient_list)\n",
    "data_gen.set_distribution_params(distribution_params)\n",
    "dataset = data_gen.make_dataset()\n",
    "\n",
    "# Training and Result Visualization\n",
    "th1_list, th0_list, cost_list = trainer(dataset, th1, th0, lr, epochs, batch_size)\n",
    "result_visualizer(th1_list, th0_list, cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_06.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "\n",
    "다음의 조건\n",
    "- initial $\\theta_{1}, \\theta_{0}$ = 0.1, 0.1\n",
    "- learning rate = 0.01\n",
    "\n",
    "\n",
    "은 고정으로 두고, dataset의 std와 batch size를 변화시켜가며 학습이 되는 경향을 확인하세요.  \n",
    "이때 batch size는 epoch당 iteration에 변화를 만드므로 epoch도 알맞게 조절해줘야합니다. \n",
    "\n",
    "- mean = 0, 1, 2, 3, -1, -2, -3\n",
    "- std = 1\n",
    "- batch size = 1, 2, 8, 16, 32, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "\n",
    "batch_size = \n",
    "\n",
    "coefficient_list = \n",
    "distribution_params = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "\n",
    "# Dataset Generation\n",
    "data_gen = LR_dataset_generator(feature_dim = 1)\n",
    "data_gen.set_coefficient(coefficient_list)\n",
    "data_gen.set_distribution_params(distribution_params)\n",
    "dataset = data_gen.make_dataset()\n",
    "\n",
    "# Training and Result Visualization\n",
    "th1_list, th0_list, cost_list = trainer(dataset, th1, th0, lr, epochs, batch_size)\n",
    "result_visualizer(th1_list, th0_list, cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_08_07.png' width=600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
